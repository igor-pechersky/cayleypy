#!/usr/bin/env python3
"""
Performance comparison for N=10 between CPU and MPS GPU.

This tests whether GPU becomes competitive for larger problem sizes.
"""

import sys
import time
import torch
import gc
from contextlib import contextmanager

sys.path.insert(0, '.')
from cayleypy import CayleyGraph, PermutationGroups

@contextmanager
def timer(description):
    """Context manager to time operations with memory cleanup."""
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()
    gc.collect()
    
    start = time.perf_counter()
    yield
    end = time.perf_counter()
    
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()
    gc.collect()
    
    print(f"{description}: {(end - start) * 1000:.1f} ms")

def estimate_problem_size(N):
    """Estimate the problem size for different permutation groups."""
    import math
    
    print(f"=== PROBLEM SIZE ESTIMATION FOR N={N} ===")
    
    # Different permutation groups have different sizes
    groups = [
        ("Coxeter", "Symmetric group generated by adjacent transpositions"),
        ("LRX", "Left shift, Right shift, eXchange generators"),
        ("Pancake", "Prefix reversal generators"),
    ]
    
    for name, description in groups:
        print(f"\n{name}({N}):")
        print(f"  Description: {description}")
        
        if name == "Coxeter":
            # Coxeter generates the full symmetric group S_N
            size = math.factorial(N)
            print(f"  Expected vertices: {size:,} (full S_{N})")
        elif name == "LRX":
            # LRX also generates full S_N but may have different diameter
            size = math.factorial(N)
            print(f"  Expected vertices: {size:,} (full S_{N})")
        elif name == "Pancake":
            # Pancake also generates full S_N
            size = math.factorial(N)
            print(f"  Expected vertices: {size:,} (full S_{N})")
        
        # Memory estimation
        memory_per_state = N * 8  # 8 bytes per int64 element
        total_memory_mb = (size * memory_per_state) / (1024 * 1024)
        print(f"  Estimated memory: {total_memory_mb:.1f} MB")
    
    print(f"\nFor N={N}: {math.factorial(N):,} total permutations")
    print(f"This is {math.factorial(N) / math.factorial(7):.1f}x larger than N=7")

def benchmark_group(group_name, graph_def, max_diameter=None):
    """Benchmark a specific permutation group on both devices."""
    print(f"\n=== {group_name} BENCHMARK ===")
    
    results = {}
    
    for device in ["cpu", "mps"]:
        print(f"\nDevice: {device}")
        
        try:
            # Create graph
            with timer(f"  Graph creation"):
                graph = CayleyGraph(graph_def, device=device, verbose=0)
            
            print(f"    Encoded state size: {graph.encoded_state_size}")
            print(f"    String encoder: {graph.string_encoder is not None}")
            print(f"    Batch size: {graph.batch_size}")
            
            # Warm up GPU if needed
            if device == "mps":
                print("    Warming up GPU...")
                try:
                    _ = graph.bfs(max_diameter=2)  # Small warmup
                except:
                    pass
            
            # Main benchmark
            bfs_kwargs = {}
            if max_diameter:
                bfs_kwargs['max_diameter'] = max_diameter
            
            with timer(f"  BFS execution"):
                result = graph.bfs(**bfs_kwargs)
            
            print(f"    Vertices found: {result.num_vertices:,}")
            print(f"    Diameter: {result.diameter()}")
            print(f"    BFS completed: {result.bfs_completed}")
            print(f"    Layer sizes (first 10): {result.layer_sizes[:10]}")
            if len(result.layer_sizes) > 10:
                print(f"    Layer sizes (last 5): {result.layer_sizes[-5:]}")
            
            # Memory usage
            if device == "mps":
                memory_mb = torch.mps.current_allocated_memory() / (1024 * 1024)
                print(f"    GPU memory used: {memory_mb:.1f} MB")
            
            results[device] = {
                'vertices': result.num_vertices,
                'diameter': result.diameter(),
                'completed': result.bfs_completed,
                'layers': len(result.layer_sizes)
            }
            
            # Cleanup
            del graph, result
            
        except Exception as e:
            print(f"    ERROR: {e}")
            results[device] = {'error': str(e)}
    
    # Compare results
    if 'cpu' in results and 'mps' in results and 'error' not in results['cpu'] and 'error' not in results['mps']:
        print(f"\n{group_name} COMPARISON:")
        print(f"  Results match: {results['cpu']['vertices'] == results['mps']['vertices']}")
        if results['cpu']['vertices'] == results['mps']['vertices']:
            print("  ✅ Both devices produced identical results")
        else:
            print("  ❌ Results differ between devices!")
    
    return results

def main():
    print("CayleyPy Performance Comparison: N=10")
    print("=" * 50)
    
    N = 10
    estimate_problem_size(N)
    
    # Test different permutation groups
    groups_to_test = [
        ("Coxeter(10)", PermutationGroups.coxeter(N), 15),  # Limit diameter for feasibility
        ("LRX(10)", PermutationGroups.lrx(N), 20),
        ("Pancake(10)", PermutationGroups.pancake(N), 25),
    ]
    
    all_results = {}
    
    for group_name, graph_def, max_diameter in groups_to_test:
        try:
            results = benchmark_group(group_name, graph_def, max_diameter)
            all_results[group_name] = results
        except KeyboardInterrupt:
            print(f"\n⚠️ Interrupted during {group_name}")
            break
        except Exception as e:
            print(f"\n❌ Failed {group_name}: {e}")
    
    # Summary
    print("\n" + "=" * 50)
    print("PERFORMANCE SUMMARY:")
    
    for group_name, results in all_results.items():
        print(f"\n{group_name}:")
        if 'cpu' in results and 'mps' in results:
            cpu_result = results['cpu']
            mps_result = results['mps']
            
            if 'error' not in cpu_result and 'error' not in mps_result:
                print(f"  Vertices: {cpu_result['vertices']:,}")
                print(f"  CPU vs MPS: Results {'match' if cpu_result['vertices'] == mps_result['vertices'] else 'differ'}")
            else:
                if 'error' in cpu_result:
                    print(f"  CPU error: {cpu_result['error']}")
                if 'error' in mps_result:
                    print(f"  MPS error: {mps_result['error']}")
    
    print("\nKEY INSIGHTS:")
    print("1. Check if GPU becomes competitive for larger problems")
    print("2. Look for memory limitations or timeouts")
    print("3. Compare actual computation time vs overhead")
    print("4. Identify the crossover point where GPU becomes beneficial")

if __name__ == "__main__":
    main()