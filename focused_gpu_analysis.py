#!/usr/bin/env python3
"""
Focused analysis of GPU BFS bottlenecks for S_n with all transpositions.
"""

import sys
import math
import time
import torch

sys.path.insert(0, '.')
from cayleypy import CayleyGraph, PermutationGroups

def analyze_bottlenecks():
    """Analyze specific bottlenecks in current implementation."""
    print("=== GPU BFS BOTTLENECK ANALYSIS ===\n")
    
    # Problem characteristics for S_n with all transpositions
    for n in [12, 13, 14, 15]:
        num_generators = n * (n - 1) // 2
        total_states = math.factorial(n)
        print(f"S_{n}: {num_generators} generators, {total_states:,} states")
    
    print("\n=== KEY BOTTLENECKS ===")
    
    bottlenecks = [
        "1. Sequential neighbor generation - O(nÂ²) generators processed sequentially",
        "2. torch.unique() - Known GPU performance issue, especially for large tensors",
        "3. Multiple torch.gather() calls - Memory bandwidth limited",
        "4. Hash-based deduplication - Binary search not GPU-optimal",
        "5. Frequent memory allocations - No tensor reuse"
    ]
    
    for bottleneck in bottlenecks:
        print(f"   {bottleneck}")
    
    print("\n=== OPTIMIZATION OPPORTUNITIES ===")
    
    optimizations = [
        {
            "name": "Vectorized Neighbor Generation",
            "description": "Generate all neighbors in single vectorized operation",
            "speedup": "5-10x for neighbor generation",
            "complexity": "Low - pure PyTorch"
        },
        {
            "name": "Custom GPU Hash Table",
            "description": "Replace torch.unique with GPU hash table (cuco library)",
            "speedup": "3-5x for deduplication",
            "complexity": "Medium - external library integration"
        },
        {
            "name": "Fused CUDA Kernels",
            "description": "Single kernel for neighbor+hash+dedupe",
            "speedup": "10-20x overall",
            "complexity": "High - custom CUDA development"
        },
        {
            "name": "Memory Pool",
            "description": "Reuse tensors to reduce allocation overhead",
            "speedup": "1.5-2x allocation speedup",
            "complexity": "Low - Python implementation"
        }
    ]
    
    for opt in optimizations:
        print(f"\n{opt['name']}:")
        print(f"  Description: {opt['description']}")
        print(f"  Expected Speedup: {opt['speedup']}")
        print(f"  Complexity: {opt['complexity']}")

def demonstrate_vectorized_neighbors():
    """Show how to implement vectorized neighbor generation."""
    print("\n=== VECTORIZED NEIGHBOR GENERATION DEMO ===")
    
    code = '''
# Current implementation (sequential):
def get_neighbors_current(self, states):
    states_num = states.shape[0]
    neighbors = torch.zeros((states_num * self.n_generators, states.shape[1]), 
                           dtype=torch.int64, device=self.device)
    for i in range(self.n_generators):
        dst = neighbors[i * states_num : (i + 1) * states_num, :]
        move = self.permutations_torch[i].reshape((1, -1)).expand(states_num, -1)
        dst[:, :] = torch.gather(states, 1, move)
    return neighbors

# Optimized implementation (vectorized):
def get_neighbors_optimized(self, states):
    batch_size, n = states.shape
    n_gen = self.n_generators
    
    # Stack all generators: (n_gen, n)
    all_generators = torch.stack(self.permutations_torch)
    
    # Expand states for all generators: (batch, n_gen, n)
    states_expanded = states.unsqueeze(1).expand(-1, n_gen, -1)
    
    # Expand generators for all states: (batch, n_gen, n)
    generators_expanded = all_generators.unsqueeze(0).expand(batch_size, -1, -1)
    
    # Single vectorized gather: (batch, n_gen, n)
    neighbors = torch.gather(states_expanded, 2, generators_expanded)
    
    # Reshape to (batch * n_gen, n)
    return neighbors.reshape(-1, n)
'''
    
    print(code)
    
    print("\nKey improvements:")
    print("- Single torch.gather() call instead of n(n-1)/2 calls")
    print("- Better GPU utilization through vectorization")
    print("- Reduced memory bandwidth requirements")

def estimate_performance_impact():
    """Estimate performance improvements."""
    print("\n=== PERFORMANCE IMPACT ESTIMATES ===")
    
    print("For S_15 (105 generators, 1.3T states):")
    print("\nCurrent bottleneck breakdown:")
    print("  Neighbor generation: ~40% of time")
    print("  Deduplication (torch.unique): ~35% of time") 
    print("  Hash operations: ~15% of time")
    print("  Memory operations: ~10% of time")
    
    print("\nOptimization scenarios:")
    
    scenarios = [
        ("Quick wins (vectorized + memory pool)", "3-5x overall speedup"),
        ("Medium effort (+ GPU hash table)", "8-15x overall speedup"),
        ("Full optimization (+ custom CUDA)", "20-50x overall speedup")
    ]
    
    for scenario, speedup in scenarios:
        print(f"  {scenario}: {speedup}")
    
    print("\nImplementation priority:")
    print("1. Vectorized neighbors (1 week, low risk, 3x speedup)")
    print("2. GPU hash table (2-3 weeks, medium risk, 8x speedup)")
    print("3. Custom CUDA kernels (4-6 weeks, high risk, 20x+ speedup)")

if __name__ == "__main__":
    analyze_bottlenecks()
    demonstrate_vectorized_neighbors()
    estimate_performance_impact()