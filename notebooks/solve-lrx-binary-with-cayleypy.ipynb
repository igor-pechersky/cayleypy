{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/cayleypy/cayleypy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T04:56:44.393558Z","iopub.execute_input":"2025-07-10T04:56:44.393727Z","iopub.status.idle":"2025-07-10T04:56:54.893192Z","shell.execute_reply.started":"2025-07-10T04:56:44.393704Z","shell.execute_reply":"2025-07-10T04:56:54.892406Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/cayleypy/cayleypy\n  Cloning https://github.com/cayleypy/cayleypy to /tmp/pip-req-build-g98ko1gd\n  Running command git clone --filter=blob:none --quiet https://github.com/cayleypy/cayleypy /tmp/pip-req-build-g98ko1gd\n  Resolved https://github.com/cayleypy/cayleypy to commit e005e674275114e49bf84c273d50623c4f2b0d28\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from cayleypy==0.1.0) (3.14.0)\nRequirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from cayleypy==0.1.0) (0.60.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from cayleypy==0.1.0) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from cayleypy==0.1.0) (1.15.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->cayleypy==0.1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->cayleypy==0.1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->cayleypy==0.1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->cayleypy==0.1.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->cayleypy==0.1.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->cayleypy==0.1.0) (2.4.1)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->cayleypy==0.1.0) (0.43.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->cayleypy==0.1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->cayleypy==0.1.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->cayleypy==0.1.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->cayleypy==0.1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->cayleypy==0.1.0) (2024.2.0)\nBuilding wheels for collected packages: cayleypy\n  Building wheel for cayleypy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for cayleypy: filename=cayleypy-0.1.0-py3-none-any.whl size=121436 sha256=906dfa172bd6aa1ce4ccf5a68d515d9d03b232c5e2148fc94fe351ec4cc0bd38\n  Stored in directory: /tmp/pip-ephem-wheel-cache-v406410q/wheels/5d/b5/53/86782f2010b218369465580e31a6c289b7f2a73390b39a23f3\nSuccessfully built cayleypy\nInstalling collected packages: cayleypy\nSuccessfully installed cayleypy-0.1.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from cayleypy import PermutationGroups, CayleyGraph, Predictor, prepare_graph\n\n\ngraph32=CayleyGraph(PermutationGroups.lrx(32).with_central_state([0]*16+[1]*16))\nX, y = graph32.random_walks(width=1000, length=250, mode=\"bfs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T05:08:53.759464Z","iopub.execute_input":"2025-07-10T05:08:53.759735Z","iopub.status.idle":"2025-07-10T05:08:54.146843Z","shell.execute_reply.started":"2025-07-10T05:08:53.759712Z","shell.execute_reply":"2025-07-10T05:08:54.146138Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\n\n\n# Training parameters.\nhidden_dims = [64]\nlearning_rate = 0.005\n\n\nclass Net(torch.nn.Module):\n    def __init__(self, input_size, hidden_dims):\n        super().__init__()\n\n        layers = []\n        for hidden_dim in hidden_dims:\n            layers.append(torch.nn.Linear(input_size, hidden_dim))\n            layers.append(torch.nn.GELU())\n            input_size = hidden_dim\n            \n        layers.append(torch.nn.Linear(input_size, 1))\n        self.layers = torch.nn.Sequential(*layers)\n\n    def forward(self, x):\n        #x = torch.nn.functional.one_hot(x.long(), num_classes=self.num_classes).float().flatten(start_dim=-2)\n        return self.layers(x.float()).squeeze(-1)\n\ninput_size = graph32.definition.state_size\nmodel = Net(input_size, hidden_dims).to(graph32.device)\n\n# Prepare training and validation datasets.\nval_ratio = 0.1\nbatch_size = 1024\ndataset = TensorDataset(X, y.float())\nval_size = int(len(dataset) * val_ratio)\ntrain_size = len(dataset)-val_size\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\ntrain_loader = DataLoader(train_ds, batch_size=1024, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=batch_size)\n\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nepoch = [0]\ndef train_one_epoch():\n    model.train()\n    total_train_loss = 0\n    for xb, yb in train_loader:\n        pred = model(xb)\n        loss = loss_fn(pred, yb)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_train_loss += loss.item() * xb.size(0)\n\n    model.eval()\n    total_val_loss = 0\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            pred = model(xb)\n            loss = loss_fn(pred, yb)\n            total_val_loss += loss.item() * xb.size(0)\n\n    avg_train_loss = total_train_loss / train_size\n    avg_val_loss = total_val_loss / val_size\n    print(f\"Epoch {epoch[0]} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n    epoch[0] +=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T05:09:00.945194Z","iopub.execute_input":"2025-07-10T05:09:00.945486Z","iopub.status.idle":"2025-07-10T05:09:00.978099Z","shell.execute_reply.started":"2025-07-10T05:09:00.945463Z","shell.execute_reply":"2025-07-10T05:09:00.977219Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"train_one_epoch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T05:09:02.670247Z","iopub.execute_input":"2025-07-10T05:09:02.671217Z","iopub.status.idle":"2025-07-10T05:09:04.863264Z","shell.execute_reply.started":"2025-07-10T05:09:02.671190Z","shell.execute_reply":"2025-07-10T05:09:04.862545Z"}},"outputs":[{"name":"stdout","text":"Epoch 0 | Train Loss: 8576.5223 | Val Loss: 3943.3830\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"train_one_epoch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T05:09:11.608929Z","iopub.execute_input":"2025-07-10T05:09:11.609258Z","iopub.status.idle":"2025-07-10T05:09:13.903547Z","shell.execute_reply.started":"2025-07-10T05:09:11.609236Z","shell.execute_reply":"2025-07-10T05:09:13.902860Z"}},"outputs":[{"name":"stdout","text":"Epoch 2 | Train Loss: 1452.5528 | Val Loss: 1142.8181\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"for _ in range(50):\n    train_one_epoch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T05:09:40.335835Z","iopub.execute_input":"2025-07-10T05:09:40.336465Z","iopub.status.idle":"2025-07-10T05:11:38.401368Z","shell.execute_reply.started":"2025-07-10T05:09:40.336437Z","shell.execute_reply":"2025-07-10T05:11:38.400686Z"}},"outputs":[{"name":"stdout","text":"Epoch 13 | Train Loss: 1056.6506 | Val Loss: 1044.3301\nEpoch 14 | Train Loss: 1051.7265 | Val Loss: 1045.8320\nEpoch 15 | Train Loss: 1047.7202 | Val Loss: 1034.7199\nEpoch 16 | Train Loss: 1044.3524 | Val Loss: 1035.0812\nEpoch 17 | Train Loss: 1041.3847 | Val Loss: 1026.8484\nEpoch 18 | Train Loss: 1037.2906 | Val Loss: 1024.7876\nEpoch 19 | Train Loss: 1034.1304 | Val Loss: 1022.9436\nEpoch 20 | Train Loss: 1032.0311 | Val Loss: 1023.3606\nEpoch 21 | Train Loss: 1030.7028 | Val Loss: 1023.8673\nEpoch 22 | Train Loss: 1027.2247 | Val Loss: 1019.1542\nEpoch 23 | Train Loss: 1024.3880 | Val Loss: 1012.9774\nEpoch 24 | Train Loss: 1022.9767 | Val Loss: 1010.8688\nEpoch 25 | Train Loss: 1020.4800 | Val Loss: 1010.0855\nEpoch 26 | Train Loss: 1019.1123 | Val Loss: 1013.5585\nEpoch 27 | Train Loss: 1017.4808 | Val Loss: 1006.4413\nEpoch 28 | Train Loss: 1016.0414 | Val Loss: 1005.6515\nEpoch 29 | Train Loss: 1014.4582 | Val Loss: 1004.9050\nEpoch 30 | Train Loss: 1013.2301 | Val Loss: 1006.6377\nEpoch 31 | Train Loss: 1011.3646 | Val Loss: 1000.6902\nEpoch 32 | Train Loss: 1012.9421 | Val Loss: 1000.7084\nEpoch 33 | Train Loss: 1009.7481 | Val Loss: 999.9654\nEpoch 34 | Train Loss: 1007.0360 | Val Loss: 997.5566\nEpoch 35 | Train Loss: 1005.4216 | Val Loss: 997.2414\nEpoch 36 | Train Loss: 1005.1394 | Val Loss: 996.6889\nEpoch 37 | Train Loss: 1003.6554 | Val Loss: 995.4411\nEpoch 38 | Train Loss: 1002.3022 | Val Loss: 993.7949\nEpoch 39 | Train Loss: 1001.8992 | Val Loss: 995.1873\nEpoch 40 | Train Loss: 1000.1941 | Val Loss: 996.0829\nEpoch 41 | Train Loss: 999.7428 | Val Loss: 990.1329\nEpoch 42 | Train Loss: 997.5691 | Val Loss: 992.3689\nEpoch 43 | Train Loss: 996.6749 | Val Loss: 995.6500\nEpoch 44 | Train Loss: 995.9578 | Val Loss: 988.5218\nEpoch 45 | Train Loss: 993.5729 | Val Loss: 990.2923\nEpoch 46 | Train Loss: 992.6105 | Val Loss: 987.6483\nEpoch 47 | Train Loss: 992.3983 | Val Loss: 987.1984\nEpoch 48 | Train Loss: 992.4562 | Val Loss: 985.3958\nEpoch 49 | Train Loss: 992.0706 | Val Loss: 986.5184\nEpoch 50 | Train Loss: 992.0580 | Val Loss: 988.1779\nEpoch 51 | Train Loss: 990.6533 | Val Loss: 988.4451\nEpoch 52 | Train Loss: 988.3191 | Val Loss: 983.1987\nEpoch 53 | Train Loss: 988.4459 | Val Loss: 985.3355\nEpoch 54 | Train Loss: 987.5882 | Val Loss: 993.2752\nEpoch 55 | Train Loss: 986.3124 | Val Loss: 980.9911\nEpoch 56 | Train Loss: 986.5028 | Val Loss: 980.4400\nEpoch 57 | Train Loss: 986.3032 | Val Loss: 986.5164\nEpoch 58 | Train Loss: 986.1615 | Val Loss: 980.2766\nEpoch 59 | Train Loss: 984.3006 | Val Loss: 982.4241\nEpoch 60 | Train Loss: 984.6242 | Val Loss: 978.3170\nEpoch 61 | Train Loss: 983.0778 | Val Loss: 977.3281\nEpoch 62 | Train Loss: 982.4784 | Val Loss: 980.2007\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"for _ in range(100):\n    train_one_epoch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T05:11:38.402573Z","iopub.execute_input":"2025-07-10T05:11:38.402847Z","iopub.status.idle":"2025-07-10T05:15:26.178102Z","shell.execute_reply.started":"2025-07-10T05:11:38.402828Z","shell.execute_reply":"2025-07-10T05:15:26.177308Z"}},"outputs":[{"name":"stdout","text":"Epoch 63 | Train Loss: 982.7765 | Val Loss: 982.7830\nEpoch 64 | Train Loss: 984.4134 | Val Loss: 978.9164\nEpoch 65 | Train Loss: 982.2243 | Val Loss: 983.1977\nEpoch 66 | Train Loss: 981.2681 | Val Loss: 977.6310\nEpoch 67 | Train Loss: 980.7106 | Val Loss: 976.4590\nEpoch 68 | Train Loss: 980.0582 | Val Loss: 978.7453\nEpoch 69 | Train Loss: 979.9697 | Val Loss: 980.9239\nEpoch 70 | Train Loss: 980.0337 | Val Loss: 975.2041\nEpoch 71 | Train Loss: 979.1939 | Val Loss: 980.2762\nEpoch 72 | Train Loss: 977.8342 | Val Loss: 974.5480\nEpoch 73 | Train Loss: 979.0592 | Val Loss: 981.5770\nEpoch 74 | Train Loss: 977.7321 | Val Loss: 973.4996\nEpoch 75 | Train Loss: 978.9430 | Val Loss: 977.5574\nEpoch 76 | Train Loss: 977.7048 | Val Loss: 978.6529\nEpoch 77 | Train Loss: 976.3333 | Val Loss: 972.6416\nEpoch 78 | Train Loss: 977.3602 | Val Loss: 978.7624\nEpoch 79 | Train Loss: 976.3525 | Val Loss: 972.6823\nEpoch 80 | Train Loss: 975.8973 | Val Loss: 971.8137\nEpoch 81 | Train Loss: 975.5642 | Val Loss: 972.6684\nEpoch 82 | Train Loss: 975.1696 | Val Loss: 972.7474\nEpoch 83 | Train Loss: 975.1008 | Val Loss: 972.1301\nEpoch 84 | Train Loss: 974.0668 | Val Loss: 974.0420\nEpoch 85 | Train Loss: 976.5235 | Val Loss: 970.2963\nEpoch 86 | Train Loss: 973.8945 | Val Loss: 969.4958\nEpoch 87 | Train Loss: 974.5900 | Val Loss: 974.4333\nEpoch 88 | Train Loss: 973.6777 | Val Loss: 969.8472\nEpoch 89 | Train Loss: 972.3799 | Val Loss: 967.2296\nEpoch 90 | Train Loss: 972.8502 | Val Loss: 972.6910\nEpoch 91 | Train Loss: 972.3117 | Val Loss: 967.3783\nEpoch 92 | Train Loss: 974.4449 | Val Loss: 975.6715\nEpoch 93 | Train Loss: 972.0881 | Val Loss: 968.9809\nEpoch 94 | Train Loss: 970.5550 | Val Loss: 966.4230\nEpoch 95 | Train Loss: 971.2659 | Val Loss: 966.6104\nEpoch 96 | Train Loss: 970.4687 | Val Loss: 966.3457\nEpoch 97 | Train Loss: 968.9171 | Val Loss: 966.5540\nEpoch 98 | Train Loss: 971.3576 | Val Loss: 970.1741\nEpoch 99 | Train Loss: 969.6900 | Val Loss: 967.1352\nEpoch 100 | Train Loss: 968.7823 | Val Loss: 964.5557\nEpoch 101 | Train Loss: 969.1493 | Val Loss: 964.4870\nEpoch 102 | Train Loss: 968.2741 | Val Loss: 966.1762\nEpoch 103 | Train Loss: 967.6591 | Val Loss: 964.1195\nEpoch 104 | Train Loss: 968.1034 | Val Loss: 962.6742\nEpoch 105 | Train Loss: 968.0878 | Val Loss: 964.0346\nEpoch 106 | Train Loss: 968.2303 | Val Loss: 964.2164\nEpoch 107 | Train Loss: 968.1010 | Val Loss: 965.8064\nEpoch 108 | Train Loss: 968.0150 | Val Loss: 961.4449\nEpoch 109 | Train Loss: 967.6011 | Val Loss: 961.1989\nEpoch 110 | Train Loss: 967.9299 | Val Loss: 964.8860\nEpoch 111 | Train Loss: 966.6424 | Val Loss: 964.8890\nEpoch 112 | Train Loss: 965.6510 | Val Loss: 960.8367\nEpoch 113 | Train Loss: 968.1486 | Val Loss: 963.6661\nEpoch 114 | Train Loss: 966.0971 | Val Loss: 963.0251\nEpoch 115 | Train Loss: 965.3528 | Val Loss: 962.9436\nEpoch 116 | Train Loss: 966.2356 | Val Loss: 963.2827\nEpoch 117 | Train Loss: 965.5159 | Val Loss: 961.1472\nEpoch 118 | Train Loss: 964.7778 | Val Loss: 964.5095\nEpoch 119 | Train Loss: 965.3959 | Val Loss: 961.7386\nEpoch 120 | Train Loss: 964.5326 | Val Loss: 960.1960\nEpoch 121 | Train Loss: 965.1376 | Val Loss: 962.0581\nEpoch 122 | Train Loss: 964.7492 | Val Loss: 963.4479\nEpoch 123 | Train Loss: 964.3295 | Val Loss: 958.0689\nEpoch 124 | Train Loss: 963.6686 | Val Loss: 961.2749\nEpoch 125 | Train Loss: 964.1910 | Val Loss: 966.0831\nEpoch 126 | Train Loss: 963.4773 | Val Loss: 958.9372\nEpoch 127 | Train Loss: 963.1171 | Val Loss: 959.3769\nEpoch 128 | Train Loss: 963.8574 | Val Loss: 957.9271\nEpoch 129 | Train Loss: 962.6513 | Val Loss: 957.9946\nEpoch 130 | Train Loss: 962.7953 | Val Loss: 961.6978\nEpoch 131 | Train Loss: 962.8186 | Val Loss: 957.4486\nEpoch 132 | Train Loss: 963.2375 | Val Loss: 956.2976\nEpoch 133 | Train Loss: 962.0079 | Val Loss: 958.7079\nEpoch 134 | Train Loss: 962.0201 | Val Loss: 964.7155\nEpoch 135 | Train Loss: 962.4410 | Val Loss: 958.0216\nEpoch 136 | Train Loss: 961.8809 | Val Loss: 955.4826\nEpoch 137 | Train Loss: 961.2931 | Val Loss: 955.6304\nEpoch 138 | Train Loss: 961.0933 | Val Loss: 956.4733\nEpoch 139 | Train Loss: 962.0309 | Val Loss: 967.6345\nEpoch 140 | Train Loss: 961.1004 | Val Loss: 956.6423\nEpoch 141 | Train Loss: 960.9265 | Val Loss: 965.5078\nEpoch 142 | Train Loss: 960.2386 | Val Loss: 957.8118\nEpoch 143 | Train Loss: 961.4751 | Val Loss: 957.0940\nEpoch 144 | Train Loss: 961.1680 | Val Loss: 955.6705\nEpoch 145 | Train Loss: 960.2354 | Val Loss: 955.5851\nEpoch 146 | Train Loss: 960.0739 | Val Loss: 964.0986\nEpoch 147 | Train Loss: 961.8032 | Val Loss: 956.9848\nEpoch 148 | Train Loss: 959.7760 | Val Loss: 955.1545\nEpoch 149 | Train Loss: 959.0888 | Val Loss: 958.6075\nEpoch 150 | Train Loss: 959.6690 | Val Loss: 957.2469\nEpoch 151 | Train Loss: 959.5437 | Val Loss: 966.8333\nEpoch 152 | Train Loss: 959.3170 | Val Loss: 955.2915\nEpoch 153 | Train Loss: 958.6961 | Val Loss: 956.2462\nEpoch 154 | Train Loss: 959.1782 | Val Loss: 957.2925\nEpoch 155 | Train Loss: 958.9418 | Val Loss: 954.9199\nEpoch 156 | Train Loss: 958.0321 | Val Loss: 961.9569\nEpoch 157 | Train Loss: 959.4732 | Val Loss: 964.1309\nEpoch 158 | Train Loss: 957.5016 | Val Loss: 953.3530\nEpoch 159 | Train Loss: 958.0002 | Val Loss: 954.0676\nEpoch 160 | Train Loss: 957.6135 | Val Loss: 956.2119\nEpoch 161 | Train Loss: 957.3259 | Val Loss: 957.6347\nEpoch 162 | Train Loss: 957.2957 | Val Loss: 955.1389\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import pandas as pd\nfrom cayleypy import PermutationGroups, CayleyGraph, Predictor\nimport torch\n\nbaseline = pd.read_csv('/kaggle/working/ans0.csv')\n\n\n\nGEN_MAP={\"L\":0, \"R\":1, \"X\":2}\n\ndef validate(graph, start_state, sol):\n    result = graph.apply_path(start_state, sol)\n    assert torch.equal(result[0], graph.central_state)\n    #print(\"OK\")\n\ntotal_delta=0\ntotal_score=0\nans=[\"permutation,solution\"]\nfor i, row in baseline.iterrows():\n    perm_str = row[\"permutation\"]\n    start_state = [int(x) for x in perm_str.split(\",\")]\n    sol0 = [GEN_MAP[x] for x in row[\"solution\"].split(\".\")]\n    score0 = len(sol0)\n\n    n = len(start_state)\n\n    sol = sol0\n    #print(\"n=\", n)\n    if n == 32:\n        #graph = CayleyGraph(PermutationGroups.lrx(n).with_central_state([0]*(n//2) + [1]*(n//2)))\n        bs_result=graph32.beam_search(start_state=start_state, predictor=Predictor(graph32, model),\n                                      return_path=True, beam_width=200000, max_iterations=score0)\n        assert bs_result.path_found\n        sol = bs_result.path\n        validate(graph32, start_state, sol) \n        delta = len(sol) - len(sol0)\n        print(i, \"n=\", n, \"score=\", len(sol), \"delta=\", delta)\n        total_delta+=delta\n    else:\n        pass\n\n    total_score+=len(sol)\n    sol_encoded = \".\".join(\"LRX\"[i] for i in sol)\n    ans.append(f'\"{perm_str}\",{sol_encoded}')\n\nprint(\"total_delta=\", total_delta)\nprint(\"total_score=\", total_score)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T05:41:02.429673Z","iopub.execute_input":"2025-07-10T05:41:02.429976Z","iopub.status.idle":"2025-07-10T05:44:45.500683Z","shell.execute_reply.started":"2025-07-10T05:41:02.429952Z","shell.execute_reply":"2025-07-10T05:44:45.499753Z"}},"outputs":[{"name":"stdout","text":"100 n= 32 score= 258 delta= -103\n101 n= 32 score= 219 delta= -78\n102 n= 32 score= 207 delta= -87\n103 n= 32 score= 275 delta= -114\n104 n= 32 score= 197 delta= -92\n105 n= 32 score= 287 delta= -140\n106 n= 32 score= 256 delta= -111\n107 n= 32 score= 244 delta= -85\n108 n= 32 score= 303 delta= -132\n109 n= 32 score= 278 delta= -131\n110 n= 32 score= 209 delta= -96\n111 n= 32 score= 333 delta= -165\n112 n= 32 score= 268 delta= -140\n113 n= 32 score= 289 delta= -129\n114 n= 32 score= 237 delta= -118\n115 n= 32 score= 233 delta= -72\n116 n= 32 score= 293 delta= -123\n117 n= 32 score= 266 delta= -120\n118 n= 32 score= 223 delta= -76\n119 n= 32 score= 280 delta= -133\n120 n= 32 score= 273 delta= -140\n121 n= 32 score= 239 delta= -83\n122 n= 32 score= 284 delta= -116\n123 n= 32 score= 236 delta= -108\n124 n= 32 score= 220 delta= -72\n125 n= 32 score= 230 delta= -64\n126 n= 32 score= 253 delta= -91\n127 n= 32 score= 219 delta= -56\n128 n= 32 score= 298 delta= -131\n129 n= 32 score= 221 delta= -87\n130 n= 32 score= 185 delta= -96\n131 n= 32 score= 222 delta= -64\n132 n= 32 score= 265 delta= -107\n133 n= 32 score= 254 delta= -136\n134 n= 32 score= 292 delta= -137\n135 n= 32 score= 216 delta= -93\n136 n= 32 score= 237 delta= -109\n137 n= 32 score= 299 delta= -131\n138 n= 32 score= 204 delta= -113\n139 n= 32 score= 243 delta= -91\n140 n= 32 score= 260 delta= -123\n141 n= 32 score= 205 delta= -97\n142 n= 32 score= 245 delta= -111\n143 n= 32 score= 226 delta= -83\n144 n= 32 score= 246 delta= -117\n145 n= 32 score= 283 delta= -111\n146 n= 32 score= 281 delta= -113\n147 n= 32 score= 244 delta= -101\n148 n= 32 score= 260 delta= -112\n149 n= 32 score= 298 delta= -156\n150 n= 32 score= 253 delta= -112\n151 n= 32 score= 205 delta= -105\n152 n= 32 score= 255 delta= -101\n153 n= 32 score= 238 delta= -89\n154 n= 32 score= 279 delta= -131\n155 n= 32 score= 257 delta= -95\n156 n= 32 score= 227 delta= -95\n157 n= 32 score= 279 delta= -107\n158 n= 32 score= 318 delta= -147\n159 n= 32 score= 269 delta= -144\n160 n= 32 score= 254 delta= -107\n161 n= 32 score= 125 delta= -44\n162 n= 32 score= 280 delta= -146\n163 n= 32 score= 194 delta= -81\n164 n= 32 score= 204 delta= -78\n165 n= 32 score= 191 delta= -86\n166 n= 32 score= 259 delta= -104\n167 n= 32 score= 193 delta= -82\n168 n= 32 score= 228 delta= -99\n169 n= 32 score= 244 delta= -125\n170 n= 32 score= 269 delta= -105\n171 n= 32 score= 222 delta= -100\n172 n= 32 score= 209 delta= -66\n173 n= 32 score= 278 delta= -112\n174 n= 32 score= 220 delta= -76\n175 n= 32 score= 246 delta= -106\n176 n= 32 score= 236 delta= -95\n177 n= 32 score= 239 delta= -93\n178 n= 32 score= 293 delta= -127\n179 n= 32 score= 284 delta= -126\n180 n= 32 score= 309 delta= -165\n181 n= 32 score= 290 delta= -120\n182 n= 32 score= 265 delta= -93\n183 n= 32 score= 250 delta= -94\n184 n= 32 score= 229 delta= -103\n185 n= 32 score= 280 delta= -128\n186 n= 32 score= 221 delta= -102\n187 n= 32 score= 178 delta= -63\n188 n= 32 score= 256 delta= -107\n189 n= 32 score= 282 delta= -117\n190 n= 32 score= 197 delta= -82\n191 n= 32 score= 204 delta= -65\n192 n= 32 score= 288 delta= -129\n193 n= 32 score= 251 delta= -136\n194 n= 32 score= 254 delta= -118\n195 n= 32 score= 191 delta= -128\n196 n= 32 score= 271 delta= -144\n197 n= 32 score= 257 delta= -136\n198 n= 32 score= 282 delta= -138\n199 n= 32 score= 286 delta= -140\ntotal_delta= -10786\ntotal_score= 2710594\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"with open(\"/kaggle/working/ans1.csv\", \"w\") as f:\n    f.write(\"\\n\".join(ans))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T05:44:54.089180Z","iopub.execute_input":"2025-07-10T05:44:54.089462Z","iopub.status.idle":"2025-07-10T05:44:54.100808Z","shell.execute_reply.started":"2025-07-10T05:44:54.089439Z","shell.execute_reply":"2025-07-10T05:44:54.100197Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}