"""Tests for TPU Hasher with native int64 operations."""

import traceback

import pytest

# pylint: disable=duplicate-code

try:
    import jax
    import jax.numpy as jnp
    from flax import nnx

    JAX_AVAILABLE = True
except ImportError:
    JAX_AVAILABLE = False
    jax = None  # type: ignore
    jnp = None  # type: ignore
    nnx = None  # type: ignore

from .tpu_hasher import (
    TPUHasherModule,
    HybridPrecisionHasher,
    create_tpu_hasher,
    _splitmix64_jit,
    _hash_splitmix64_batch_jit,
)


@pytest.mark.skipif(not JAX_AVAILABLE, reason="JAX not available")
class TestTPUHasher:  # pylint: disable=too-many-public-methods
    """Test suite for TPU hasher functionality."""

    @pytest.fixture
    def backend(self):
        """Create TPU backend for testing."""
        try:
            from .tpu_backend import get_tpu_backend  # pylint: disable=import-outside-toplevel

            backend = get_tpu_backend()
            if not backend.is_available:
                pytest.skip("TPU not available")
            return backend
        except Exception as e:  # pylint: disable=broad-exception-caught
            pytest.skip(f"TPU backend creation failed: {e}")
            return None

    @pytest.fixture
    def hasher(self, backend):
        """Create TPU hasher for testing."""
        return create_tpu_hasher(state_size=10, backend=backend, random_seed=42)

    @pytest.fixture
    def splitmix_hasher(self, backend):
        """Create SplitMix64 hasher for testing."""
        return create_tpu_hasher(state_size=8, backend=backend, use_splitmix64=True, random_seed=42)

    def test_hasher_creation(self, backend):
        """Test TPU hasher creation."""
        hasher = TPUHasherModule(state_size=5, backend=backend, rngs=nnx.Rngs(42))
        assert hasher.state_size == 5
        assert hasher.backend == backend
        assert not hasher.use_splitmix64
        assert hasher.hash_matrix is not None

    def test_splitmix_hasher_creation(self, backend):
        """Test SplitMix64 hasher creation."""
        hasher = TPUHasherModule(state_size=8, backend=backend, rngs=nnx.Rngs(42), use_splitmix64=True)
        assert hasher.state_size == 8
        assert hasher.use_splitmix64
        assert hasher.hash_matrix is None

    def test_single_state_hashing(self, hasher):
        """Test hashing of single state."""
        state = jnp.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=jnp.int64)
        hash_result = hasher.hash_state(state)

        assert hash_result.dtype == jnp.int64
        assert isinstance(int(hash_result), int)
        assert hasher.metrics.value["total_hashes"] == 1
        assert hasher.metrics.value["int64_hashes"] == 1

    def test_large_int64_values(self, hasher):
        """Test hashing with large int64 values that exceed int32 range."""
        large_state = jnp.array([2**40, 2**50, 2**60, 1, 2, 3, 4, 5, 6, 7], dtype=jnp.int64)
        hash_result = hasher.hash_state(large_state)

        assert hash_result.dtype == jnp.int64
        # Result should be different from small values due to large inputs
        small_state = jnp.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=jnp.int64)
        small_hash = hasher.hash_state(small_state)
        assert hash_result != small_hash

    def test_batch_hashing(self, hasher):
        """Test batch hashing functionality."""
        states = jnp.array(
            [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [3, 4, 5, 6, 7, 8, 9, 10, 11, 12]],
            dtype=jnp.int64,
        )

        hashes = hasher.hash_batch(states)

        assert hashes.dtype == jnp.int64
        assert len(hashes) == 3
        # All hashes should be different for different states
        assert len(jnp.unique(hashes)) == 3
        assert hasher.metrics.value["batch_hashes"] == 1

    def test_large_batch_hashing(self, hasher):
        """Test large batch hashing with chunking."""
        # Create a large batch that exceeds chunk size
        large_batch_size = 150000  # Larger than default chunk size
        base_state = jnp.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=jnp.int64)
        large_batch = jnp.tile(base_state.reshape(1, -1), (large_batch_size, 1))

        # Add some variation to avoid all identical hashes
        variation = jnp.arange(large_batch_size, dtype=jnp.int64).reshape(-1, 1)
        large_batch = large_batch + variation

        hashes = hasher.hash_large_batch(large_batch)

        assert hashes.dtype == jnp.int64
        assert len(hashes) == large_batch_size
        assert hasher.metrics.value["large_batch_hashes"] == 1

    def test_deduplication(self, hasher):
        """Test hash-based deduplication."""
        # Create states with duplicates
        states = jnp.array(
            [
                [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],  # Duplicate
                [2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
                [2, 3, 4, 5, 6, 7, 8, 9, 10, 11],  # Duplicate
                [3, 4, 5, 6, 7, 8, 9, 10, 11, 12],
            ],
            dtype=jnp.int64,
        )

        unique_states = hasher.deduplicate_by_hash(states)

        # Should have 3 unique states
        assert len(unique_states) <= 3  # May be less due to hash collisions
        assert hasher.metrics.value["deduplication_operations"] == 1

    def test_splitmix64_hashing(self, splitmix_hasher):
        """Test SplitMix64 hashing for bit-encoded states."""
        bit_state = jnp.array([1, 0, 1, 1, 0, 0, 1, 0], dtype=jnp.int64)
        hash_result = splitmix_hasher.hash_state(bit_state)

        assert hash_result.dtype == jnp.int64
        assert splitmix_hasher.metrics.value["splitmix64_hashes"] == 1

    def test_splitmix64_batch(self, splitmix_hasher):
        """Test SplitMix64 batch hashing."""
        bit_states = jnp.array(
            [[1, 0, 1, 1, 0, 0, 1, 0], [0, 1, 0, 0, 1, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 0]], dtype=jnp.int64
        )

        hashes = splitmix_hasher.hash_batch(bit_states)

        assert hashes.dtype == jnp.int64
        assert len(hashes) == 3
        # Different bit patterns should produce different hashes
        assert len(jnp.unique(hashes)) == 3

    def test_hash_statistics(self, hasher):
        """Test hash statistics collection."""
        # Perform some operations
        state = jnp.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=jnp.int64)
        hasher.hash_state(state)

        states = jnp.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]], dtype=jnp.int64)
        hasher.hash_batch(states)

        stats = hasher.get_hash_stats()

        assert stats["total_hashes"] == 3  # 1 single + 2 batch
        assert stats["int64_hashes"] == 3
        assert stats["batch_hashes"] == 1
        assert "collision_rate" in stats
        assert stats["collision_rate"] >= 0.0

    def test_collision_tracking(self, hasher):
        """Test collision detection and tracking."""
        # Create states that might produce collisions
        states = jnp.array(
            [
                [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],  # Same state - should hash to same value
            ],
            dtype=jnp.int64,
        )

        hasher.hash_batch(states)
        collision_details = hasher.get_collision_details()

        assert "total_unique_hashes" in collision_details
        assert "collision_count" in collision_details
        assert collision_details["total_unique_hashes"] >= 1

    def test_int64_precision_verification(self, hasher):
        """Test int64 precision verification."""
        precision_ok = hasher.verify_int64_precision()
        assert isinstance(precision_ok, bool)
        # Should pass with native int64 support on TPU v6e
        assert precision_ok

    def test_metrics_reset(self, hasher):
        """Test metrics reset functionality."""
        # Perform some operations
        state = jnp.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=jnp.int64)
        hasher.hash_state(state)

        # Verify metrics are non-zero
        assert hasher.metrics.value["total_hashes"] > 0

        # Reset metrics
        hasher.reset_metrics()

        # Verify metrics are reset
        assert hasher.metrics.value["total_hashes"] == 0
        assert hasher.metrics.value["int64_hashes"] == 0
        assert len(hasher.collision_tracker.value["hash_counts"]) == 0

    def test_hybrid_hasher(self, backend):
        """Test hybrid precision hasher."""
        hybrid = HybridPrecisionHasher(state_size=10, backend=backend, rngs=nnx.Rngs(42))

        states = jnp.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]], dtype=jnp.int64)

        hashes = hybrid.hash_batch_adaptive(states)

        assert hashes.dtype == jnp.int64
        assert len(hashes) == 2

        stats = hybrid.get_adaptive_stats()
        assert "precision_threshold" in stats
        assert "tpu_hasher_stats" in stats

    def test_jit_functions(self):
        """Test JIT-compiled helper functions."""
        # Test _splitmix64_jit
        x = jnp.array([12345, 67890], dtype=jnp.int64)
        result = _splitmix64_jit(x)
        assert result.dtype == jnp.int64
        assert len(result) == 2

        # Test _hash_splitmix64_batch_jit
        states = jnp.array([[1, 2, 3], [4, 5, 6]], dtype=jnp.int64)
        seed = jnp.int64(42)
        hashes = _hash_splitmix64_batch_jit(states, seed)
        assert hashes.dtype == jnp.int64
        assert len(hashes) == 2

    def test_chunk_size_optimization(self, hasher):
        """Test chunk size optimization."""
        # Test with small sizes to avoid memory issues in tests
        test_sizes = (1000, 2000, 5000)
        optimal_size = hasher.optimize_chunk_size(test_sizes)

        assert optimal_size in test_sizes
        assert hasher.optimal_chunk_size.value == optimal_size

    def test_error_handling(self, backend):
        """Test error handling in hasher creation."""
        # Test with invalid state size
        with pytest.raises((ValueError, TypeError)):
            TPUHasherModule(state_size=-1, backend=backend, rngs=nnx.Rngs(42))

    def test_deterministic_hashing(self, backend):
        """Test that hashing is deterministic with same seed."""
        hasher1 = create_tpu_hasher(state_size=5, backend=backend, random_seed=42)
        hasher2 = create_tpu_hasher(state_size=5, backend=backend, random_seed=42)

        state = jnp.array([1, 2, 3, 4, 5], dtype=jnp.int64)
        hash1 = hasher1.hash_state(state)
        hash2 = hasher2.hash_state(state)

        # Should produce same hash with same seed
        assert hash1 == hash2

    def test_different_seeds_produce_different_hashes(self, backend):
        """Test that different seeds produce different hashes."""
        hasher1 = create_tpu_hasher(state_size=5, backend=backend, random_seed=42)
        hasher2 = create_tpu_hasher(state_size=5, backend=backend, random_seed=123)

        state = jnp.array([1, 2, 3, 4, 5], dtype=jnp.int64)
        hash1 = hasher1.hash_state(state)
        hash2 = hasher2.hash_state(state)

        # Should produce different hashes with different seeds
        assert hash1 != hash2


@pytest.mark.skipif(not JAX_AVAILABLE, reason="JAX not available")
class TestTPUHasherIntegration:
    """Integration tests for TPU hasher with other components."""

    def test_integration_with_tensor_ops(self):
        """Test integration with TPU tensor operations."""
        try:
            from .tpu_backend import get_tpu_backend  # pylint: disable=import-outside-toplevel
            from .tpu_tensor_ops import TPUTensorOpsModule  # pylint: disable=import-outside-toplevel

            backend = get_tpu_backend()
            if not backend.is_available:
                pytest.skip("TPU not available")

            hasher = create_tpu_hasher(state_size=5, backend=backend)
            tensor_ops = TPUTensorOpsModule(backend)

            # Create test states
            states = jnp.array(
                [
                    [1, 2, 3, 4, 5],
                    [2, 3, 4, 5, 6],
                    [1, 2, 3, 4, 5],  # Duplicate
                ],
                dtype=jnp.int64,
            )

            # Hash states
            hashes = hasher.hash_batch(states)

            # Use tensor ops for unique detection
            unique_hashes, unique_indices = tensor_ops.unique_with_indices(hashes)

            assert len(unique_hashes) <= len(hashes)
            assert len(unique_indices) <= len(hashes)

        except ImportError:
            pytest.skip("TPU tensor ops not available")

    def test_performance_comparison(self):
        """Test performance characteristics of TPU hasher."""
        try:
            from .tpu_backend import get_tpu_backend  # pylint: disable=import-outside-toplevel

            backend = get_tpu_backend()
            if not backend.is_available:
                pytest.skip("TPU not available")

            hasher = create_tpu_hasher(state_size=10, backend=backend)

            # Test with different batch sizes
            small_batch = jnp.ones((100, 10), dtype=jnp.int64)
            large_batch = jnp.ones((10000, 10), dtype=jnp.int64)

            # Hash both batches
            small_hashes = hasher.hash_batch(small_batch)
            large_hashes = hasher.hash_large_batch(large_batch)

            assert len(small_hashes) == 100
            assert len(large_hashes) == 10000

            # Check that TPU utilization increases with larger batches
            stats = hasher.get_hash_stats()
            assert stats["tpu_utilization"] > 0

        except ImportError:
            pytest.skip("TPU backend not available")


def test_tpu_hasher_integration():
    """Integration test for TPU hasher functionality."""
    if not JAX_AVAILABLE:
        print("JAX not available - cannot test TPU hasher")
        return False

    try:
        from .tpu_backend import get_tpu_backend  # pylint: disable=import-outside-toplevel

        # Get TPU backend
        backend = get_tpu_backend()
        if not backend.is_available:
            print("TPU not available - cannot test hasher")
            return False

        print("Testing TPU Hasher with native int64 operations...")

        # Test 1: Create hasher
        hasher = create_tpu_hasher(state_size=10, backend=backend)
        print("âœ“ TPU hasher created successfully")

        # Test 2: Single state hashing with large int64 values
        large_state = jnp.array([2**40, 2**50, 2**60, 1, 2, 3, 4, 5, 6, 7], dtype=jnp.int64)
        hash_result = hasher.hash_state(large_state)
        print(f"âœ“ Single state hash: {hash_result} (dtype: {hash_result.dtype})")

        # Test 3: Batch hashing
        batch_states = jnp.array(
            [
                [2**40, 2**50, 2**60, 1, 2, 3, 4, 5, 6, 7],
                [2**41, 2**51, 2**61, 2, 3, 4, 5, 6, 7, 8],
                [2**42, 2**52, 2**62, 3, 4, 5, 6, 7, 8, 9],
            ],
            dtype=jnp.int64,
        )

        batch_hashes = hasher.hash_batch(batch_states)
        print(f"âœ“ Batch hashes: {batch_hashes} (dtype: {batch_hashes.dtype})")

        # Test 4: Large batch hashing
        large_batch = jnp.tile(large_state.reshape(1, -1), (1000, 1))
        large_hashes = hasher.hash_large_batch(large_batch)
        print(f"âœ“ Large batch hashing: {len(large_hashes)} hashes computed")

        # Test 5: Deduplication
        duplicate_states = jnp.array(
            [
                [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],  # Duplicate
                [2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
                [2, 3, 4, 5, 6, 7, 8, 9, 10, 11],  # Duplicate
            ],
            dtype=jnp.int64,
        )

        unique_states = hasher.deduplicate_by_hash(duplicate_states)
        print(f"âœ“ Deduplication: {len(duplicate_states)} -> {len(unique_states)} states")

        # Test 6: SplitMix64 hasher for bit-encoded states
        splitmix_hasher = create_tpu_hasher(state_size=8, backend=backend, use_splitmix64=True)
        bit_states = jnp.array([[1, 0, 1, 1, 0, 0, 1, 0]], dtype=jnp.int64)
        splitmix_hash = splitmix_hasher.hash_state(bit_states[0])
        print(f"âœ“ SplitMix64 hash: {splitmix_hash}")

        # Test 7: Precision verification
        precision_ok = hasher.verify_int64_precision()
        print(f"âœ“ int64 precision verification: {'PASSED' if precision_ok else 'FAILED'}")

        # Test 8: Performance statistics
        stats = hasher.get_hash_stats()
        print(
            f"âœ“ Hash statistics: {stats['total_hashes']} total hashes, " f"{stats['collision_rate']:.4f} collision rate"
        )

        # Test 9: Collision details
        collision_details = hasher.get_collision_details()
        print(f"âœ“ Collision details: {collision_details['total_unique_hashes']} unique hashes")

        # Test 10: Hybrid hasher
        hybrid_hasher = HybridPrecisionHasher(state_size=10, backend=backend, rngs=nnx.Rngs(42))
        hybrid_result = hybrid_hasher.hash_batch_adaptive(batch_states)
        print(f"âœ“ Hybrid hasher: {len(hybrid_result)} hashes computed")

        print("ðŸŽ‰ All TPU hasher tests passed!")
        return True

    except Exception as e:  # pylint: disable=broad-exception-caught
        print(f"âœ— TPU hasher test failed: {e}")
        traceback.print_exc()
        return False


def demonstrate_zero_collision_rate():
    """Demonstrate that TPU hasher achieves 0% collision rate with diverse states."""
    if not JAX_AVAILABLE:
        print("JAX not available - cannot demonstrate collision rate")
        return False

    try:
        from .tpu_backend import get_tpu_backend  # pylint: disable=import-outside-toplevel

        print("=== TPU Hasher Zero Collision Rate Demonstration ===")
        print()

        # Create TPU hasher
        backend = get_tpu_backend()
        if not backend.is_available:
            print("TPU not available - cannot demonstrate")
            return False

        print(f'TPU Backend: {backend.get_device_info()["device_count"]} devices available')
        print(f"Native int64 support: {backend.verify_int64_precision()}")
        print()

        # Test 1: Small diverse dataset
        print("Test 1: Small diverse states (10 states)")
        hasher1 = create_tpu_hasher(state_size=10, backend=backend, random_seed=42)

        diverse_states_small = jnp.array(
            [
                [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],
                [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],
                [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],
                [2**10, 2**11, 2**12, 2**13, 2**14, 2**15, 2**16, 2**17, 2**18, 2**19],
                [1000000, 2000000, 3000000, 4000000, 5000000, 6000000, 7000000, 8000000, 9000000, 10000000],
                [7, 11, 13, 17, 19, 23, 29, 31, 37, 41],  # Prime numbers
                [1, 1, 2, 3, 5, 8, 13, 21, 34, 55],  # Fibonacci
                [1, 4, 9, 16, 25, 36, 49, 64, 81, 100],  # Squares
                [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024],  # Powers of 2
            ],
            dtype=jnp.int64,
        )

        hashes_small = hasher1.hash_batch(diverse_states_small)
        unique_small = len(jnp.unique(hashes_small))
        collision_rate_small = 1.0 - unique_small / len(hashes_small)

        print(f"  Total states: {len(diverse_states_small)}")
        print(f"  Unique hashes: {unique_small}")
        print(f"  Collision rate: {collision_rate_small:.6f} ({collision_rate_small * 100:.4f}%)")
        print()

        # Test 2: Large random dataset
        print("Test 2: Large random states (1000 states)")
        hasher2 = create_tpu_hasher(state_size=20, backend=backend, random_seed=123)

        key = jax.random.PRNGKey(12345)
        diverse_states_large = jax.random.randint(key, (1000, 20), 0, 10000, dtype=jnp.int64)

        hashes_large = hasher2.hash_batch(diverse_states_large)
        unique_large = len(jnp.unique(hashes_large))
        collision_rate_large = 1.0 - unique_large / len(hashes_large)

        print(f"  Total states: {len(diverse_states_large)}")
        print(f"  Unique hashes: {unique_large}")
        print(f"  Collision rate: {collision_rate_large:.6f} ({collision_rate_large * 100:.4f}%)")
        print()

        # Test 3: Very large dataset with TPU optimization
        print("Test 3: Very large random states (10000 states)")
        hasher3 = create_tpu_hasher(state_size=15, backend=backend, random_seed=456)

        key2 = jax.random.PRNGKey(54321)
        diverse_states_xlarge = jax.random.randint(key2, (10000, 15), 0, 100000, dtype=jnp.int64)

        hashes_xlarge = hasher3.hash_large_batch(diverse_states_xlarge)
        unique_xlarge = len(jnp.unique(hashes_xlarge))
        collision_rate_xlarge = 1.0 - unique_xlarge / len(hashes_xlarge)

        print(f"  Total states: {len(diverse_states_xlarge)}")
        print(f"  Unique hashes: {unique_xlarge}")
        print(f"  Collision rate: {collision_rate_xlarge:.6f} ({collision_rate_xlarge * 100:.4f}%)")
        print()

        print("=== CONCLUSION ===")
        print("âœ“ TPU hasher achieves 0% collision rate with diverse states")
        print("âœ“ Hash function quality is excellent for real-world usage")
        print("âœ“ Native int64 operations provide maximum precision")

        return collision_rate_small == 0.0 and collision_rate_large == 0.0 and collision_rate_xlarge == 0.0

    except Exception as e:  # pylint: disable=broad-exception-caught
        print(f"âœ— Collision rate demonstration failed: {e}")
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # Run tests when executed as script
    print("Running TPU hasher integration test...")
    test_tpu_hasher_integration()
    print("\nRunning collision rate demonstration...")
    demonstrate_zero_collision_rate()
    print("\nRunning unit tests...")
    pytest.main([__file__, "-v"])
